# Model Configuration
model:
  d_model: 512          # Model dimension
  n_heads: 8            # Number of attention heads
  n_layers: 6           # Number of encoder/decoder layers
  d_ff: 2048           # Feed-forward dimension
  dropout: 0.1         # Dropout rate
  max_seq_len: 512     # Maximum sequence length
  vocab_size_src: 32000  # Source vocabulary size
  vocab_size_tgt: 32000  # Target vocabulary size
  
# Positional Encoding Configuration
positional_encoding:
  type: "rope"  # Options: "rope", "relative_bias"
  
# Training Configuration
training:
  batch_size: 16
  learning_rate: 0.0001
  num_epochs: 50
  warmup_steps: 4000
  label_smoothing: 0.1
  gradient_clip: 1.0
  save_every: 5  # Save checkpoint every N epochs
  
# Data Configuration
data:
  train_src: "EUbookshop.fi"
  train_tgt: "EUbookshop.en"
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  max_len: 256
  
# Decoding Configuration
decoding:
  strategy: "top_k"  # Options: "greedy", "beam", "top_k"
  beam_size: 5        # For beam search
  k: 50              # For top-k sampling
  temperature: 1.0    # For sampling
  max_length: 256
  
# File Paths
paths:
  model_save_path: "checkpoints/"
  log_path: "logs/"
  vocab_path: "vocab/"